{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å¾®è°ƒæ¨¡åž‹\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/data/study/python/NLP/.env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (â€¦)/main/tokenizer.json: 466kB [00:00, 1.35MB/s]\n",
      "Downloading model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 440M/440M [00:21<00:00, 20.5MB/s] \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Volumes/data/study/python/NLP/.env/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Same as before\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"This course is amazing!\",\n",
    "]\n",
    "batch = tokenizer(sequences, padding=True,\n",
    "                  truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# This is new\n",
    "batch[\"labels\"] = torch.tensor([1, 1])\n",
    "\n",
    "optimizer = AdamW(model.parameters())\n",
    "loss = model(**batch).loss\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
       "          2607,  2026,  2878,  2166,  1012,   102],\n",
       "        [  101,  2023,  2607,  2003,  6429,   999,   102,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([1, 1])}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å½“ç„¶ï¼Œä»…ä»…ç”¨ä¸¤å¥è¯è®­ç»ƒæ¨¡åž‹ä¸ä¼šäº§ç”Ÿå¾ˆå¥½çš„æ•ˆæžœã€‚ä¸ºäº†èŽ·å¾—æ›´å¥½çš„ç»“æžœï¼Œæ‚¨éœ€è¦å‡†å¤‡ä¸€ä¸ªæ›´å¤§çš„æ•°æ®é›†ã€‚\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä»Žæ¨¡åž‹ä¸­å¿ƒï¼ˆHubï¼‰åŠ è½½æ•°æ®é›†\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è®©æˆ‘ä»¬ä½¿ç”¨ MRPC æ•°æ®é›†ä¸­çš„[GLUE åŸºå‡†æµ‹è¯•æ•°æ®é›†](https://gluebenchmark.com/)ï¼Œå®ƒæ˜¯æž„æˆ MRPC æ•°æ®é›†çš„ 10 ä¸ªæ•°æ®é›†ä¹‹ä¸€ï¼Œè¿™æ˜¯ä¸€ä¸ªå­¦æœ¯åŸºå‡†ï¼Œç”¨äºŽè¡¡é‡æœºå™¨å­¦ä¹ æ¨¡åž‹åœ¨ 10 ä¸ªä¸åŒæ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/Users/elton/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 777.59it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "raw_datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "é»˜è®¤æƒ…å†µä¸‹ï¼Œæ­¤å‘½ä»¤åœ¨ä¸‹è½½æ•°æ®é›†å¹¶ç¼“å­˜åˆ° `~/.cache/huggingface/datasets` .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
       " 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n",
       " 'label': 1,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset = raw_datasets[\"train\"]\n",
    "raw_train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': Value(dtype='string', id=None),\n",
       " 'sentence2': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None),\n",
       " 'idx': Value(dtype='int32', id=None)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset.features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨ä¸Šé¢çš„ä¾‹å­ä¹‹ä¸­, `Label` ï¼ˆæ ‡ç­¾ï¼‰ æ˜¯ä¸€ç§ ClassLabelï¼ˆåˆ†ç±»æ ‡ç­¾ï¼‰ï¼Œä½¿ç”¨æ•´æ•°å»ºç«‹èµ·åˆ°ç±»åˆ«æ ‡ç­¾çš„æ˜ å°„å…³ç³»ã€‚0 å¯¹åº”äºŽ `not_equivalent` ï¼Œ1 å¯¹åº”äºŽ `equivalent` ã€‚\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## é¢„å¤„ç†æ•°æ®\n",
    "\n",
    "`tokenize_function` å‡½æ•°å°†æ•°æ®é›†ä¸­çš„æ¯ä¸ªç¤ºä¾‹é€šè¿‡ `tokenizer`\n",
    "\n",
    "è¯·æ³¨æ„ï¼Œæˆ‘ä»¬çŽ°åœ¨åœ¨ `tokenize_function` æ ‡è®°å‡½æ•°ä¸­çœç•¥äº† `padding` å‚æ•°ã€‚è¿™æ˜¯å› ä¸ºåœ¨æ ‡è®°çš„æ—¶å€™å°†æ‰€æœ‰æ ·æœ¬å¡«å……åˆ°æœ€å¤§é•¿åº¦çš„æ•ˆçŽ‡ä¸é«˜ã€‚ä¸€ä¸ªæ›´å¥½çš„åšæ³•ï¼šåœ¨æž„å»ºæ‰¹å¤„ç†æ—¶å¡«å……æ ·æœ¬æ›´å¥½ï¼Œå› ä¸ºè¿™æ ·æˆ‘ä»¬åªéœ€è¦å¡«å……åˆ°è¯¥æ‰¹å¤„ç†ä¸­çš„æœ€å¤§é•¿åº¦ï¼Œè€Œä¸æ˜¯æ•´ä¸ªæ•°æ®é›†çš„æœ€å¤§é•¿åº¦ã€‚å½“è¾“å…¥é•¿åº¦å˜åŒ–å¾ˆå¤§æ—¶ï¼Œè¿™å¯ä»¥èŠ‚çœå¤§é‡æ—¶é—´å’Œå¤„ç†èƒ½åŠ›!\n",
    "\n",
    "ä½¿ç”¨ [Dataset.map()](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map) æ–¹æ³•, å¹¶ä½¿ç”¨ `batched=True` å‚æ•°ï¼Œè¿™æ ·å‡½æ•°å°±å¯ä»¥åŒæ—¶åº”ç”¨åˆ°æ•°æ®é›†çš„å¤šä¸ªå…ƒç´ ä¸Šï¼Œè€Œä¸æ˜¯åˆ†åˆ«åº”ç”¨åˆ°æ¯ä¸ªå…ƒç´ ä¸Šï¼Œè¿™å°†æ˜¾è‘—åŠ å¿«æ ‡è®°ä¸Žæ ‡è®°çš„é€Ÿåº¦ã€‚è¿™ä¸ªæ ‡è®°å™¨æ¥è‡ª [ðŸ¤— Tokenizers åº“](https://github.com/huggingface/tokenizers) ç”± Rust ç¼–å†™è€Œæˆã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/elton/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-bb21e6423b980722.arrow\n",
      "Loading cached processed dataset at /Users/elton/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-d1e8c90b5d349f7a.arrow\n",
      "Loading cached processed dataset at /Users/elton/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-33304e37c309912f.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¯ä»¥çœ‹åˆ°æ¯”ä¹‹å‰çš„ `raw_datasets` æ•°æ®é›†ï¼Œ `tokenized_datasets` æ•°æ®é›†å·²ç»è¢«æ ‡è®°äº†ï¼Œæ–°å¢žäº† `input_ids` ï¼Œ `token_type_ids` å’Œ `attention_mask` ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
       " 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n",
       " 'label': 1,\n",
       " 'idx': 0,\n",
       " 'input_ids': [101,\n",
       "  2572,\n",
       "  3217,\n",
       "  5831,\n",
       "  5496,\n",
       "  2010,\n",
       "  2567,\n",
       "  1010,\n",
       "  3183,\n",
       "  2002,\n",
       "  2170,\n",
       "  1000,\n",
       "  1996,\n",
       "  7409,\n",
       "  1000,\n",
       "  1010,\n",
       "  1997,\n",
       "  9969,\n",
       "  4487,\n",
       "  23809,\n",
       "  3436,\n",
       "  2010,\n",
       "  3350,\n",
       "  1012,\n",
       "  102,\n",
       "  7727,\n",
       "  2000,\n",
       "  2032,\n",
       "  2004,\n",
       "  2069,\n",
       "  1000,\n",
       "  1996,\n",
       "  7409,\n",
       "  1000,\n",
       "  1010,\n",
       "  2572,\n",
       "  3217,\n",
       "  5831,\n",
       "  5496,\n",
       "  2010,\n",
       "  2567,\n",
       "  1997,\n",
       "  9969,\n",
       "  4487,\n",
       "  23809,\n",
       "  3436,\n",
       "  2010,\n",
       "  3350,\n",
       "  1012,\n",
       "  102],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### åŠ¨æ€å¡«å……\n",
    "\n",
    "ä¸ºäº†è§£å†³è¾“å…¥çš„å¥å­é•¿åº¦ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œæˆ‘ä»¬éœ€è¦å¯¹è¾“å…¥è¿›è¡Œå¡«å……ã€‚ä½†æ˜¯æ²¡æœ‰ç”¨ `padding` å‚æ•°ï¼Œå› ä¸ºè¿™æ ·åšæ•ˆçŽ‡ä¸é«˜ã€‚ä¸€ä¸ªæ›´å¥½çš„åšæ³•æ˜¯åœ¨æž„å»ºæ‰¹å¤„ç†æ—¶å¡«å……æ ·æœ¬ï¼Œå› ä¸ºè¿™æ ·æˆ‘ä»¬åªéœ€è¦å¡«å……åˆ°è¯¥æ‰¹å¤„ç†ä¸­çš„æœ€å¤§é•¿åº¦ï¼Œè€Œä¸æ˜¯æ•´ä¸ªæ•°æ®é›†çš„æœ€å¤§é•¿åº¦ã€‚å½“è¾“å…¥é•¿åº¦å˜åŒ–å¾ˆå¤§æ—¶ï¼Œè¿™å¯ä»¥èŠ‚çœå¤§é‡æ—¶é—´å’Œå¤„ç†èƒ½åŠ›!\n",
    "\n",
    "ðŸ¤—transformer åº“é€šè¿‡ `DataCollatorWithPadding` ä¸ºæˆ‘ä»¬æä¾›äº†è¿™æ ·ä¸€ä¸ªå‡½æ•°ã€‚å½“ä½ å®žä¾‹åŒ–å®ƒæ—¶ï¼Œéœ€è¦ä¸€ä¸ªæ ‡è®°å™¨(ç”¨æ¥çŸ¥é“ä½¿ç”¨å“ªä¸ªè¯æ¥å¡«å……ï¼Œä»¥åŠæ¨¡åž‹æœŸæœ›å¡«å……åœ¨å·¦è¾¹è¿˜æ˜¯å³è¾¹)ï¼Œå¹¶å°†åšä½ éœ€è¦çš„ä¸€åˆ‡:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬å¯ä»¥æŠ½å–å‡ ä¸ªæ ·ä¾‹æ¥çœ‹ä¸€ä¸‹æˆ‘ä»¬çš„ `input_ids` çš„é•¿åº¦æ˜¯ä¸æ˜¯ä¸€è‡´çš„ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50, 59, 47, 67, 59, 50, 62, 32]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = tokenized_datasets[\"train\"][:8]\n",
    "samples = {k: v for k, v in samples.items() if k not in [\n",
    "    \"idx\", \"sentence1\", \"sentence2\"]}\n",
    "[len(x) for x in samples[\"input_ids\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¯«æ— ç–‘é—®ï¼Œæˆ‘ä»¬å¾—åˆ°äº†ä¸åŒé•¿åº¦çš„æ ·æœ¬ï¼Œä»Ž 32 åˆ° 67ã€‚åŠ¨æ€å¡«å……æ„å‘³ç€è¯¥æ‰¹ä¸­çš„æ‰€æœ‰æ ·æœ¬éƒ½åº”è¯¥å¡«å……åˆ°é•¿åº¦ä¸º 67ï¼Œè¿™æ˜¯è¯¥æ‰¹ä¸­çš„æœ€å¤§é•¿åº¦ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([8, 67]),\n",
       " 'token_type_ids': torch.Size([8, 67]),\n",
       " 'attention_mask': torch.Size([8, 67]),\n",
       " 'labels': torch.Size([8])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator(samples)\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¯ä»¥çœ‹åˆ°ï¼Œé€šè¿‡ `DataCollatorWithPadding` å‡½æ•°ï¼Œå·²ç»åŠ¨æ€çš„å°†æ ·æœ¬å¡«å……åˆ°äº†æœ€å¤§é•¿åº¦ã€‚\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä½¿ç”¨ Trainer API å¾®è°ƒæ¨¡åž‹\n",
    "\n",
    "ðŸ¤— Transformers æä¾›äº†ä¸€ä¸ª Trainer ç±»æ¥å¸®åŠ©æ‚¨åœ¨è‡ªå·±çš„æ•°æ®é›†ä¸Šå¾®è°ƒä»»ä½•é¢„è®­ç»ƒæ¨¡åž‹ã€‚\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸Šä¸€æ­¥æˆ‘ä»¬æå‰é¢„å¤„ç†æ•°æ®çš„ä»£ç \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/Users/elton/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 919.67it/s]\n",
      "Loading cached processed dataset at /Users/elton/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-bb21e6423b980722.arrow\n",
      "Loading cached processed dataset at /Users/elton/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-d1e8c90b5d349f7a.arrow\n",
      "Loading cached processed dataset at /Users/elton/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-33304e37c309912f.arrow\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)  # åŠ¨æ€å¡«å……"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è®­ç»ƒ\n",
    "\n",
    "#### 1. å®šä¹‰ TrainingArguments ç±»\n",
    "\n",
    "å®ƒå°†åŒ…å« Trainer ç”¨äºŽè®­ç»ƒå’Œè¯„ä¼°çš„æ‰€æœ‰è¶…å‚æ•°ã€‚æ‚¨å”¯ä¸€å¿…é¡»æä¾›çš„å‚æ•°æ˜¯ä¿å­˜è®­ç»ƒæ¨¡åž‹çš„ç›®å½•ï¼Œä»¥åŠè®­ç»ƒè¿‡ç¨‹ä¸­çš„æ£€æŸ¥ç‚¹ã€‚å¯¹äºŽå…¶ä½™çš„å‚æ•°ï¼Œæ‚¨å¯ä»¥ä¿ç•™é»˜è®¤å€¼ï¼Œè¿™å¯¹äºŽåŸºæœ¬å¾®è°ƒåº”è¯¥éžå¸¸æœ‰æ•ˆã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# training_args = TrainingArguments(\"test-trainer\") for Linux or windows os with GPU\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer\", use_mps_device=True)  # for M1 mac"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. å®šä¹‰æ¨¡åž‹\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint, num_labels=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨å®žä¾‹åŒ–æ­¤é¢„è®­ç»ƒæ¨¡åž‹åŽä¼šæ”¶åˆ°è­¦å‘Šã€‚è¿™æ˜¯å› ä¸º BERT æ²¡æœ‰åœ¨å¥å­å¯¹åˆ†ç±»æ–¹é¢è¿›è¡Œè¿‡é¢„è®­ç»ƒ, è¿™æ­£æ˜¯æˆ‘ä»¬çŽ°åœ¨è¦åšçš„ã€‚\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. å®šä¹‰ Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. å¼€å§‹è®­ç»ƒ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/data/study/python/NLP/.env/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "                                        \n",
      "  0%|          | 0/1377 [04:31<?, ?it/s]          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5004, 'learning_rate': 3.184458968772695e-05, 'epoch': 1.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                        \n",
      "  0%|          | 0/1377 [06:12<?, ?it/s]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2709, 'learning_rate': 1.3689179375453886e-05, 'epoch': 2.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                        \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1377/1377 [04:43<00:00,  4.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 283.6369, 'train_samples_per_second': 38.796, 'train_steps_per_second': 4.855, 'train_loss': 0.3179263140554435, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1377, training_loss=0.3179263140554435, metrics={'train_runtime': 283.6369, 'train_samples_per_second': 38.796, 'train_steps_per_second': 4.855, 'train_loss': 0.3179263140554435, 'epoch': 3.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿™å°†å¼€å§‹å¾®è°ƒï¼ˆåœ¨ GPU ä¸Šåº”è¯¥éœ€è¦å‡ åˆ†é’Ÿï¼‰ï¼Œå¹¶æ¯ 500 æ­¥æŠ¥å‘Šä¸€æ¬¡è®­ç»ƒæŸå¤±ã€‚ä½†æ˜¯ï¼Œå®ƒä¸ä¼šå‘Šè¯‰æ‚¨æ¨¡åž‹çš„æ€§èƒ½å¦‚ä½•ï¼ˆæˆ–è´¨é‡å¦‚ä½•ï¼‰ã€‚è¿™æ˜¯å› ä¸º:\n",
    "\n",
    "1. æˆ‘ä»¬æ²¡æœ‰é€šè¿‡å°†`evaluation_strategy`è®¾ç½®ä¸º`steps`(åœ¨æ¯æ¬¡æ›´æ–°å‚æ•°çš„æ—¶å€™è¯„ä¼°)æˆ–â€œepochâ€(åœ¨æ¯ä¸ª epoch ç»“æŸæ—¶è¯„ä¼°)æ¥å‘Šè¯‰ Trainer åœ¨è®­ç»ƒæœŸé—´è¿›è¡Œè¯„ä¼°ã€‚\n",
    "2. æˆ‘ä»¬æ²¡æœ‰ä¸º Trainer æä¾›ä¸€ä¸ª `compute_metrics()`å‡½æ•°æ¥ç›´æŽ¥è®¡ç®—æ¨¡åž‹çš„å¥½å(å¦åˆ™è¯„ä¼°å°†åªè¾“å‡º lossï¼Œè¿™ä¸æ˜¯ä¸€ä¸ªéžå¸¸ç›´è§‚çš„æ•°å­—)ã€‚\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è¯„ä¼°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:02<00:00, 17.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(408, 2) (408,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "print(predictions.predictions.shape, predictions.label_ids.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predictions æ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º 408 x 2 çš„äºŒç»´æ•°ç»„ï¼ˆ408 æ˜¯æˆ‘ä»¬ä½¿ç”¨çš„æ•°æ®é›†ä¸­å…ƒç´ çš„æ•°é‡ï¼‰ã€‚è¿™äº›æ˜¯æˆ‘ä»¬ä¼ é€’ç»™ predict()çš„æ•°æ®é›†çš„æ¯ä¸ªå…ƒç´ çš„ç»“æžœ(logits)ï¼ˆæ­£å¦‚ä½ åœ¨ä¹‹å‰çš„ç« èŠ‚çœ‹åˆ°çš„æƒ…å†µï¼‰ã€‚è¦å°†æˆ‘ä»¬çš„é¢„æµ‹çš„å¯ä»¥ä¸ŽçœŸæ­£çš„æ ‡ç­¾è¿›è¡Œæ¯”è¾ƒï¼Œæˆ‘ä»¬éœ€è¦åœ¨ç¬¬äºŒä¸ªè½´ä¸Šå–æœ€å¤§å€¼çš„ç´¢å¼•:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "preds = np.argmax(predictions.predictions, axis=-1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "çŽ°åœ¨å»ºç«‹æˆ‘ä»¬çš„ `compute_metric()` å‡½æ•°æ¥è¾ƒä¸ºç›´è§‚åœ°è¯„ä¼°æ¨¡åž‹çš„å¥½åï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ [ðŸ¤— Evaluate](https://github.com/huggingface/evaluate/) åº“ä¸­çš„æŒ‡æ ‡ã€‚æˆ‘ä»¬å¯ä»¥åƒåŠ è½½æ•°æ®é›†ä¸€æ ·è½»æ¾åŠ è½½ä¸Ž MRPC æ•°æ®é›†å…³è”çš„æŒ‡æ ‡ï¼Œè¿™æ¬¡ä½¿ç”¨ `evaluate.load()` å‡½æ•°ã€‚è¿”å›žçš„å¯¹è±¡æœ‰ä¸€ä¸ª `compute()` æ–¹æ³•æˆ‘ä»¬å¯ä»¥ç”¨æ¥è¿›è¡Œåº¦é‡è®¡ç®—çš„æ–¹æ³•ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install evaluate\n",
    "# %pip install scipy sklearn\n",
    "# %pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "metric.compute(predictions=preds, references=predictions.label_ids)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æˆ‘ä»¬çš„æ¨¡åž‹åœ¨éªŒè¯é›†ä¸Šçš„å‡†ç¡®çŽ‡ä¸º 85.78%ï¼ŒF1 åˆ†æ•°ä¸º 89.97ã€‚è¿™æ˜¯ç”¨äºŽè¯„ä¼° GLUE åŸºå‡†çš„ MRPC æ•°æ®é›†ç»“æžœçš„ä¸¤ä¸ªæŒ‡æ ‡ã€‚è€Œåœ¨BERT è®ºæ–‡ä¸­å±•ç¤ºçš„åŸºç¡€æ¨¡åž‹çš„ F1 åˆ†æ•°ä¸º 88.9ã€‚é‚£æ˜¯ uncased æ¨¡åž‹ï¼Œè€Œæˆ‘ä»¬ç›®å‰æ­£åœ¨ä½¿ç”¨ cased æ¨¡åž‹ï¼Œé€šè¿‡æ”¹è¿›å¾—åˆ°äº†æ›´å¥½çš„ç»“æžœã€‚\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æœ€åŽå°†æ‰€æœ‰ä¸œè¥¿æ‰“åŒ…åœ¨ä¸€èµ·ï¼Œæˆ‘ä»¬å¾—åˆ°äº†æˆ‘ä»¬çš„ `compute_metrics()` å‡½æ•°ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸ºäº†æŸ¥çœ‹æ¨¡åž‹åœ¨æ¯ä¸ªè®­ç»ƒå‘¨æœŸç»“æŸçš„å¥½åï¼Œä¸‹é¢æ˜¯æˆ‘ä»¬å¦‚ä½•ä½¿ç”¨compute_metrics()å‡½æ•°å®šä¹‰ä¸€ä¸ªæ–°çš„ Trainer ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# training_args = TrainingArguments(\"test-trainer\", evaluation_strategy=\"epoch\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer\", use_mps_device=True, evaluation_strategy=\"epoch\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint, num_labels=2)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                        \n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "  0%|          | 0/1377 [01:54<?, ?it/s]          \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.38603249192237854, 'eval_accuracy': 0.8431372549019608, 'eval_f1': 0.8907849829351535, 'eval_runtime': 3.8506, 'eval_samples_per_second': 105.957, 'eval_steps_per_second': 13.245, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                        \n",
      "  0%|          | 0/1377 [02:03<?, ?it/s]          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5017, 'learning_rate': 3.184458968772695e-05, 'epoch': 1.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                        \n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "  0%|          | 0/1377 [03:30<?, ?it/s]          \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5124179124832153, 'eval_accuracy': 0.8553921568627451, 'eval_f1': 0.902155887230514, 'eval_runtime': 3.8952, 'eval_samples_per_second': 104.744, 'eval_steps_per_second': 13.093, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                        \n",
      "  0%|          | 0/1377 [03:46<?, ?it/s]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.292, 'learning_rate': 1.3689179375453886e-05, 'epoch': 2.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                        \n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "  0%|          | 0/1377 [05:07<?, ?it/s]           \n",
      "\u001b[A\n",
      "                                        \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1377/1377 [04:52<00:00,  4.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6257346868515015, 'eval_accuracy': 0.8676470588235294, 'eval_f1': 0.9078498293515359, 'eval_runtime': 3.9902, 'eval_samples_per_second': 102.25, 'eval_steps_per_second': 12.781, 'epoch': 3.0}\n",
      "{'train_runtime': 292.3134, 'train_samples_per_second': 37.645, 'train_steps_per_second': 4.711, 'train_loss': 0.33513128852705726, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1377, training_loss=0.33513128852705726, metrics={'train_runtime': 292.3134, 'train_samples_per_second': 37.645, 'train_steps_per_second': 4.711, 'train_loss': 0.33513128852705726, 'epoch': 3.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å®Œæ•´è®­ç»ƒè¿‡ç¨‹\n",
    "\n",
    "ä¸ä½¿ç”¨Trainerç±»çš„æƒ…å†µä¸‹èŽ·å¾—ä¸Žä¸Šä¸€èŠ‚ç›¸åŒçš„ç»“æžœã€‚\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å‰æœŸçš„ä»£ç \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/Users/elton/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 1239.09it/s]\n",
      "Loading cached processed dataset at /Users/elton/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-bb21e6423b980722.arrow\n",
      "Loading cached processed dataset at /Users/elton/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-d1e8c90b5d349f7a.arrow\n",
      "Loading cached processed dataset at /Users/elton/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-33304e37c309912f.arrow\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)  # æ ‡ç­¾åŒ–\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)  # åŠ¨æ€å¡«å……"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è®­ç»ƒå‰çš„å‡†å¤‡\n",
    "\n",
    "æˆ‘ä»¬éœ€è¦:\n",
    "\n",
    "1. åˆ é™¤ä¸Žæ¨¡åž‹ä¸æœŸæœ›çš„å€¼ç›¸å¯¹åº”çš„åˆ—ï¼ˆå¦‚sentence1å’Œsentence2åˆ—ï¼‰ã€‚\n",
    "2. å°†åˆ—å`label`é‡å‘½åä¸º`labels`ï¼ˆå› ä¸ºæ¨¡åž‹æœŸæœ›å‚æ•°æ˜¯labelsï¼‰ã€‚\n",
    "3. è®¾ç½®æ•°æ®é›†çš„æ ¼å¼ï¼Œä½¿å…¶è¿”å›ž `PyTorch` å¼ é‡è€Œä¸æ˜¯åˆ—è¡¨ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labels', 'input_ids', 'token_type_ids', 'attention_mask']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns(\n",
    "    [\"sentence1\", \"sentence2\", \"idx\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "tokenized_datasets[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], batch_size=8, collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'labels': torch.Size([8]),\n",
       " 'input_ids': torch.Size([8, 68]),\n",
       " 'token_type_ids': torch.Size([8, 68]),\n",
       " 'attention_mask': torch.Size([8, 68])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    break\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('labels', tensor([1, 1, 1, 1, 0, 0, 0, 1])), ('input_ids', tensor([[  101,  5153,  5207,  2097,  2022,  2067,  1999,  2254,  2044,  2732,\n",
       "         10454, 21863, 25441,  3957,  4182,  1012,   102,  6788,  2036,  3488,\n",
       "          2000,  2016,  2140,  3726,  5153,  5207,  2127,  2254,  2004,  2732,\n",
       "         10454, 21863, 25441,  2038,  2014,  2034,  2775,  1012,   102,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [  101,  1996, 18269,  5565,  2006,  2049,  2217,  1998, 10070,  2055,\n",
       "          2260,  3620,  1010,  2763,  7944,  2011,  1996,  2364, 13561,  1012,\n",
       "           102,  1996, 18269,  3092,  2039,  2006,  2049,  2217,  1998,  2596,\n",
       "          2000,  2031,  2042,  7944,  2055,  2871,  6199,  2011,  1996,  2364,\n",
       "         13561,  2044,  4899,  1012,   102,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [  101,  1996,  4289,  2442,  2022,  2019,  2330,  3115,  2008,  2038,\n",
       "          2042,  6246, 17673,  2011,  2019,  7587,  3858,  4781,  3029,  1010,\n",
       "          1998, 12997,  2442,  2022,  7000,  2104,  9608,  1010,  2512,  1011,\n",
       "          5860, 20026, 28230,  3408,  1012,   102,  1996,  4289,  2442,  2022,\n",
       "          6246, 17673,  2011,  2019,  7587,  3858,  4781,  3029,  1010,  1998,\n",
       "         12997,  2442,  2022,  7000,  2104,  9608,  1010,  2512,  1011,  5860,\n",
       "         20026, 28230,  3408,  1010,  2027,  2794,  1012,   102],\n",
       "        [  101,  2625,  2084,  1037,  3204,  2044, 15971,  2014,  8133,  1005,\n",
       "          1055,  3242,  2012,  1000,  3058,  4179,  6788,  1010,  1000,  4869,\n",
       "          2703,  3240,  2038,  2772,  1037,  2047,  3066,  2007,  6788,  2000,\n",
       "          3677,  1037, 13889, 12217,  2831,  2265,  1012,   102,  2625,  2084,\n",
       "          1037,  3204,  2044,  2975,  2004,  3677,  1997,  1000,  3058,  4179,\n",
       "          6788,  1010,  1000,  2703,  3240,  3530,  9432,  2000,  4888,  1037,\n",
       "         12217,  2831,  2265,  2005,  6788,  9926,  1012,   102],\n",
       "        [  101,  3960,  3246,  1010,  3040,  1997,  1996,  2028,  1011, 11197,\n",
       "          1998, 10925,  8837,  9971,  1010,  2351,  2007,  1037,  2868,  2006,\n",
       "          2010,  2227,  7483,  2074,  2706,  2044, 12964,  2010, 16919,  5798,\n",
       "          1012,   102,  3960,  3246,  1010,  1996,  3040,  1997,  2028,  1011,\n",
       "          2240, 21864,  4523,  1010,  2351,  4465,  2305,  1000,  2007,  1037,\n",
       "          2868,  2006,  2010,  2227,  1010,  1000,  2010,  2684,  2056,  7483,\n",
       "          1012,   102,     0,     0,     0,     0,     0,     0],\n",
       "        [  101, 11074,  1010,  2241,  1999,  4407,  1010, 24273,  2000,  2485,\n",
       "          1996,  7654,  1999,  1996,  2034,  4284,  1997,  2432,  1012,   102,\n",
       "         11074,  2056,  2009, 24273,  1996,  7660,  2000,  2485,  1999,  1996,\n",
       "          2034,  4284,  1997,  2432,  1010,  3395,  2000, 10738,  1998,  2034,\n",
       "          8862, 18668,  6226,  1012,   102,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [  101,  1996,  2561,  2193,  1997,  2047,  3572,  1999,  2859,  2001,\n",
       "          8491,  2084,  2531,  2005,  1996,  2353,  2154,  1999,  1037,  5216,\n",
       "          1012,   102,  2006,  6928,  1010,  1996,  2193,  1997, 18906,  2015,\n",
       "          3572,  1999,  2859,  2979,  1019,  1010,  2199,  1010,  7294,  1037,\n",
       "          2561,  1997,  1019,  1010,  5890,  2509,  1012,   102,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [  101, 17765,  2036,  2992,  2049, 12174, 11443,  4859,  2000,  2423,\n",
       "         16653,  2013,  2260, 16653,  1010,  8951,  1996,  2047,  4171,  2375,\n",
       "          1012,   102, 17765,  2787,  2000,  3623,  2049, 12174, 11443,  4859,\n",
       "          2000,  2423, 16653,  1037,  3745,  2013,  2260, 16653,  1010,  8951,\n",
       "          3522,  4171,  6094,  2004,  1037,  3078,  5387,  2369,  1996,  2693,\n",
       "          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]])), ('token_type_ids', tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])), ('attention_mask', tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]))])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.items()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### åŠ è½½æ¨¡åž‹\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6796, grad_fn=<NllLossBackward0>) torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**batch)\n",
    "print(outputs.loss, outputs.logits.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å½“æˆ‘ä»¬æä¾› labels æ—¶ï¼Œ ðŸ¤— Transformers æ¨¡åž‹éƒ½å°†è¿”å›žè¿™ä¸ªbatchçš„lossï¼Œæˆ‘ä»¬è¿˜å¾—åˆ°äº† logits(batchä¸­çš„æ¯ä¸ªè¾“å…¥æœ‰ä¸¤ä¸ªï¼Œæ‰€ä»¥å¼ é‡å¤§å°ä¸º 8 x 2)ã€‚\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¼˜åŒ–å™¨\n",
    "\n",
    "ç”±äºŽæˆ‘ä»¬è¯•å›¾è‡ªè¡Œå®žçŽ° Trainerçš„åŠŸèƒ½ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ç›¸åŒçš„ä¼˜åŒ–å™¨å’Œå­¦ä¹ çŽ‡è°ƒåº¦å™¨ã€‚Trainer ä½¿ç”¨çš„ä¼˜åŒ–å™¨æ˜¯ AdamW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/data/study/python/NLP/.env/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å­¦ä¹ çŽ‡è°ƒåº¦å™¨\n",
    "\n",
    "é»˜è®¤ä½¿ç”¨çš„å­¦ä¹ çŽ‡è°ƒåº¦å™¨åªæ˜¯ä»Žæœ€å¤§å€¼ (5e-5) åˆ° 0 çš„çº¿æ€§è¡°å‡ã€‚ ä¸ºäº†å®šä¹‰å®ƒï¼Œæˆ‘ä»¬éœ€è¦çŸ¥é“æˆ‘ä»¬è®­ç»ƒçš„æ¬¡æ•°ï¼Œå³æ‰€æœ‰æ•°æ®è®­ç»ƒçš„æ¬¡æ•°(epochs)ä¹˜ä»¥çš„æ•°æ®é‡ï¼ˆè¿™æ˜¯æˆ‘ä»¬æ‰€æœ‰è®­ç»ƒæ•°æ®çš„æ•°é‡ï¼‰ã€‚Traineré»˜è®¤æƒ…å†µä¸‹ä½¿ç”¨ä¸‰ä¸ªepochsï¼Œå› æ­¤æˆ‘ä»¬å®šä¹‰è®­ç»ƒè¿‡ç¨‹å¦‚ä¸‹:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1377\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "print(num_training_steps)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è®­ç»ƒå¾ªçŽ¯\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æžœæˆ‘ä»¬å¯ä»¥è®¿é—® GPU, æˆ‘ä»¬å°†å¸Œæœ›ä½¿ç”¨ GPU(åœ¨ CPU ä¸Šï¼Œè®­ç»ƒå¯èƒ½éœ€è¦å‡ ä¸ªå°æ—¶è€Œä¸æ˜¯å‡ åˆ†é’Ÿ)ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\n",
    "    \"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
