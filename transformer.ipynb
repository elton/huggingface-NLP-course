{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer æ¨¡å‹\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pipeline å‡½æ•°\n",
    "\n",
    "ğŸ¤— Transformers åº“ä¸­æœ€åŸºæœ¬çš„å¯¹è±¡æ˜¯ pipeline() å‡½æ•°ã€‚å®ƒå°†æ¨¡å‹ä¸å…¶å¿…è¦çš„é¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤è¿æ¥èµ·æ¥ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿé€šè¿‡ç›´æ¥è¾“å…¥ä»»ä½•æ–‡æœ¬å¹¶è·å¾—æœ€ç»ˆçš„ç­”æ¡ˆ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# é»˜è®¤æƒ…å†µä¸‹ï¼Œæ­¤pipelineé€‰æ‹©ä¸€ä¸ªç‰¹å®šçš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å·²é’ˆå¯¹è‹±è¯­æƒ…æ„Ÿåˆ†æè¿›è¡Œäº†å¾®è°ƒã€‚åˆ›å»ºåˆ†ç±»å™¨å¯¹è±¡æ—¶ï¼Œå°†ä¸‹è½½å¹¶ç¼“å­˜æ¨¡å‹ã€‚å¦‚æœæ‚¨é‡æ–°è¿è¡Œè¯¥å‘½ä»¤ï¼Œåˆ™å°†ä½¿ç”¨ç¼“å­˜çš„æ¨¡å‹ï¼Œæ— éœ€å†æ¬¡ä¸‹è½½æ¨¡å‹ã€‚\n",
    "classifier = pipeline(\"sentiment-analysis\")  # ä»…é€‚åˆè‹±æ–‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998737573623657}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"I'm happy today!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> å°†ä¸€äº›æ–‡æœ¬ä¼ é€’åˆ° pipeline æ—¶æ¶‰åŠä¸‰ä¸ªä¸»è¦æ­¥éª¤ï¼š\n",
    "\n",
    "> 1. æ–‡æœ¬è¢«é¢„å¤„ç†ä¸ºæ¨¡å‹å¯ä»¥ç†è§£çš„æ ¼å¼ã€‚\n",
    ">\n",
    "> 2. é¢„å¤„ç†çš„è¾“å…¥è¢«ä¼ é€’ç»™æ¨¡å‹ã€‚\n",
    ">\n",
    "> 3. æ¨¡å‹å¤„ç†åè¾“å‡ºæœ€ç»ˆäººç±»å¯ä»¥ç†è§£çš„ç»“æœã€‚\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline.svg)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä½¿ç”¨åˆ†è¯å™¨è¿›è¡Œé¢„å¤„ç†(Tokenizer)\n",
    "\n",
    "ä¸å…¶ä»–ç¥ç»ç½‘ç»œä¸€æ ·ï¼ŒTransformer æ¨¡å‹æ— æ³•ç›´æ¥å¤„ç†åŸå§‹æ–‡æœ¬ï¼Œ å› æ­¤æˆ‘ä»¬ç®¡é“çš„ç¬¬ä¸€æ­¥æ˜¯å°†æ–‡æœ¬è¾“å…¥è½¬æ¢ä¸ºæ¨¡å‹èƒ½å¤Ÿç†è§£çš„æ•°å­—ã€‚ ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨ tokenizer(æ ‡è®°å™¨)ï¼Œè´Ÿè´£ï¼š\n",
    "\n",
    "* å°†è¾“å…¥æ‹†åˆ†ä¸ºå•è¯ã€å­å•è¯æˆ–ç¬¦å·ï¼ˆå¦‚æ ‡ç‚¹ç¬¦å·ï¼‰ï¼Œç§°ä¸ºæ ‡è®°(token)\n",
    "* å°†æ¯ä¸ªæ ‡è®°(token)æ˜ å°„åˆ°ä¸€ä¸ªæ•´æ•°\n",
    "* æ·»åŠ å¯èƒ½å¯¹æ¨¡å‹æœ‰ç”¨çš„å…¶ä»–è¾“å…¥\n",
    "\n",
    "æ‰€æœ‰è¿™äº›é¢„å¤„ç†éƒ½éœ€è¦ä»¥ä¸æ¨¡å‹é¢„è®­ç»ƒæ—¶å®Œå…¨ç›¸åŒçš„æ–¹å¼å®Œæˆï¼Œå› æ­¤æˆ‘ä»¬é¦–å…ˆéœ€è¦ä» Model Hub ä¸­ä¸‹è½½è¿™äº›ä¿¡æ¯ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨ `AutoTokenizer` ç±»åŠå…¶ `from_pretrained()` æ–¹æ³•ã€‚\n",
    "\n",
    "ä½¿ç”¨æˆ‘ä»¬æ¨¡å‹çš„æ£€æŸ¥ç‚¹åç§°ï¼Œå®ƒå°†è‡ªåŠ¨è·å–ä¸æ¨¡å‹çš„æ ‡è®°å™¨ç›¸å…³è”çš„æ•°æ®ï¼Œå¹¶å¯¹å…¶è¿›è¡Œç¼“å­˜ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# sentiment-analysisï¼ˆæƒ…ç»ªåˆ†æï¼‰ä»»åŠ¡çš„é¢„è®­ç»ƒæ¨¡å‹\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸€æ—¦æˆ‘ä»¬æœ‰äº†æ ‡è®°å™¨(tokenizer)ï¼Œæˆ‘ä»¬å°±å¯ä»¥ç›´æ¥å°†æˆ‘ä»¬çš„å¥å­ä¼ é€’ç»™å®ƒï¼Œç„¶åæˆ‘ä»¬å°±ä¼šå¾—åˆ°ä¸€æœ¬å­—å…¸ï¼Œå®ƒå¯ä»¥æä¾›ç»™æˆ‘ä»¬çš„æ¨¡å‹ï¼å‰©ä¸‹è¦åšçš„å”¯ä¸€ä¸€ä»¶äº‹å°±æ˜¯å°†è¾“å…¥ ID åˆ—è¡¨è½¬æ¢ä¸ºå¼ é‡(tensor)ã€‚\n",
    "\n",
    "**å¼ é‡(tensor)**ï¼Œä½ å¯ä»¥æŠŠå®ƒä»¬æƒ³è±¡æˆ NumPy æ•°ç»„ã€‚NumPy æ•°ç»„å¯ä»¥æ˜¯æ ‡é‡ï¼ˆ0Dï¼‰ã€å‘é‡ï¼ˆ1Dï¼‰ã€çŸ©é˜µï¼ˆ2Dï¼‰æˆ–å…·æœ‰æ›´å¤šç»´åº¦ã€‚å®ƒå®é™…ä¸Šæ˜¯å¼ é‡ï¼›å…¶ä»– ML æ¡†æ¶çš„å¼ é‡è¡Œä¸ºç±»ä¼¼ï¼Œé€šå¸¸ä¸ NumPy æ•°ç»„ä¸€æ ·æ˜“äºå®ä¾‹åŒ–ã€‚\n",
    "\n",
    "è¦æŒ‡å®šè¦è¿”å›çš„å¼ é‡ç±»å‹ï¼ˆPyTorchã€TensorFlow æˆ– plain NumPyï¼‰ï¼Œæˆ‘ä»¬ä½¿ç”¨ `return_tensors` å‚æ•°ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True,\n",
    "                   truncation=True, return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¾“å‡ºæœ¬èº«æ˜¯ä¸€ä¸ªåŒ…å«ä¸¤ä¸ªé”®çš„å­—å…¸ï¼Œ `input_ids` å’Œ `attention_mask` ã€‚ `input_ids` åŒ…å«ä¸¤è¡Œæ•´æ•°ï¼ˆæ¯ä¸ªå¥å­ä¸€è¡Œï¼‰ï¼Œå®ƒä»¬æ˜¯æ¯ä¸ªå¥å­ä¸­æ ‡è®°çš„å”¯ä¸€æ ‡è®°ï¼ˆtokenï¼‰ã€‚\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä½¿ç”¨æ¨¡å‹\n",
    "\n",
    "æˆ‘ä»¬å¯ä»¥åƒä½¿ç”¨æ ‡è®°å™¨ä¸€æ ·ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹ã€‚ğŸ¤— Transformers æä¾›äº†ä¸€ä¸ª `AutoModel` ç±»ï¼Œè¯¥ç±»è¿˜å…·æœ‰ `from_pretrained()` æ–¹æ³•ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing DistilBertModel: ['classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16, 768])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers æ¨¡å—çš„çŸ¢é‡è¾“å‡ºé€šå¸¸è¾ƒå¤§ã€‚å®ƒé€šå¸¸æœ‰ä¸‰ä¸ªç»´åº¦ï¼š\n",
    "\n",
    "* Batch size: ä¸€æ¬¡å¤„ç†çš„åºåˆ—æ•°ï¼ˆåœ¨æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­ä¸º 2ï¼‰ã€‚\n",
    "* Sequence length: åºåˆ—çš„æ•°å€¼è¡¨ç¤ºçš„é•¿åº¦ï¼ˆåœ¨æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­ä¸º 16ï¼‰ã€‚\n",
    "* Hidden size: æ¯ä¸ªæ¨¡å‹è¾“å…¥çš„å‘é‡ç»´åº¦ï¼ˆåœ¨æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­ä¸º 768ï¼‰ã€‚\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/transformer_and_head.svg)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨æ­¤å›¾ä¸­ï¼Œæ¨¡å‹ç”±å…¶åµŒå…¥å±‚å’Œåç»­å±‚è¡¨ç¤ºã€‚åµŒå…¥å±‚å°†æ ‡è®°åŒ–è¾“å…¥ä¸­çš„æ¯ä¸ªè¾“å…¥ ID è½¬æ¢ä¸ºè¡¨ç¤ºå…³è”æ ‡è®°(token)çš„å‘é‡ã€‚åç»­å±‚ä½¿ç”¨æ³¨æ„æœºåˆ¶æ“çºµè¿™äº›å‘é‡ï¼Œä»¥ç”Ÿæˆå¥å­çš„æœ€ç»ˆè¡¨ç¤ºã€‚\n",
    "\n",
    "ğŸ¤— Transformers ä¸­æœ‰è®¸å¤šä¸åŒçš„ä½“ç³»ç»“æ„ï¼Œæ¯ç§ä½“ç³»ç»“æ„éƒ½æ˜¯å›´ç»•å¤„ç†ç‰¹å®šä»»åŠ¡è€Œè®¾è®¡çš„ã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªéè¯¦å°½çš„åˆ—è¡¨ï¼š\n",
    "\n",
    "* \\*Model (retrieve the hidden states)\n",
    "* \\*ForCausalLM\n",
    "* \\*ForMaskedLM\n",
    "* \\*ForMultipleChoice\n",
    "* \\*ForQuestionAnswering\n",
    "* \\*ForSequenceClassification\n",
    "* \\*ForTokenClassification\n",
    "* ä»¥åŠå…¶ä»– ğŸ¤—\n",
    "\n",
    "å¯¹äºæˆ‘ä»¬çš„ç¤ºä¾‹ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªå¸¦æœ‰åºåˆ—åˆ†ç±»å¤´çš„æ¨¡å‹ï¼ˆèƒ½å¤Ÿå°†å¥å­åˆ†ç±»ä¸ºè‚¯å®šæˆ–å¦å®šï¼‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å®é™…ä¸Šä¸ä¼šä½¿ç”¨ `AutoModel` ç±»ï¼Œè€Œæ˜¯ä½¿ç”¨ `AutoModelForSequenceClassification` ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç°åœ¨ï¼Œå¦‚æœæˆ‘ä»¬è§‚å¯Ÿè¾“å‡ºçš„å½¢çŠ¶ï¼Œç»´åº¦å°†ä½å¾—å¤šï¼šæ¨¡å‹å¤´å°†æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„é«˜ç»´å‘é‡ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¾“å‡ºåŒ…å«ä¸¤ä¸ªå€¼çš„å‘é‡ï¼ˆæ¯ä¸ªæ ‡ç­¾ä¸€ä¸ªï¼‰ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.logits.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å› ä¸ºæˆ‘ä»¬åªæœ‰ä¸¤ä¸ªå¥å­å’Œä¸¤ä¸ªæ ‡ç­¾ï¼Œæ‰€ä»¥æˆ‘ä»¬ä»æ¨¡å‹ä¸­å¾—åˆ°çš„ç»“æœæ˜¯ 2 x 2 çš„å½¢çŠ¶ã€‚\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å¯¹è¾“å‡ºè¿›è¡Œåå¤„ç†\n",
    "\n",
    "æˆ‘ä»¬ä»æ¨¡å‹ä¸­å¾—åˆ°çš„è¾“å‡ºå€¼æœ¬èº«å¹¶ä¸ä¸€å®šæœ‰æ„ä¹‰ã€‚æˆ‘ä»¬æ¥çœ‹çœ‹, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5607,  1.6123],\n",
      "        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(outputs.logits)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬çš„æ¨¡å‹é¢„æµ‹ç¬¬ä¸€å¥ä¸º[-1.5607, 1.6123]ï¼Œç¬¬äºŒå¥ä¸º[ 4.1692, -3.3464]ã€‚è¿™äº›ä¸æ˜¯æ¦‚ç‡ï¼Œè€Œæ˜¯ `logits` ï¼Œå³æ¨¡å‹æœ€åä¸€å±‚è¾“å‡ºçš„åŸå§‹éæ ‡å‡†åŒ–åˆ†æ•°ã€‚è¦è½¬æ¢ä¸ºæ¦‚ç‡ï¼Œå®ƒä»¬éœ€è¦ç»è¿‡ `SoftMax` å±‚ï¼ˆæ‰€æœ‰ ğŸ¤—Transformers æ¨¡å‹è¾“å‡º logitsï¼Œå› ä¸ºç”¨äºè®­ç»ƒçš„æŸè€—å‡½æ•°é€šå¸¸ä¼šå°†æœ€åçš„æ¿€æ´»å‡½æ•°ï¼ˆå¦‚ SoftMaxï¼‰ä¸å®é™…æŸè€—å‡½æ•°ï¼ˆå¦‚äº¤å‰ç†µï¼‰èåˆï¼‰ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.0195e-02, 9.5981e-01],\n",
      "        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç°åœ¨æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œ\n",
    "\n",
    "æ¨¡å‹é¢„æµ‹\n",
    "\n",
    "ç¬¬ä¸€å¥ä¸º[0.0402, 0.9598]ï¼Œ\n",
    "\n",
    "ç¬¬äºŒå¥ä¸º[0.9995, 0.0005]ã€‚\n",
    "\n",
    "è¿™äº›æ˜¯å¯è¯†åˆ«çš„æ¦‚ç‡åˆ†æ•°ã€‚\n",
    "\n",
    "ä¸ºäº†è·å¾—æ¯ä¸ªä½ç½®å¯¹åº”çš„å…·ä½“å«ä¹‰ï¼Œæˆ‘ä»¬å¯ä»¥æ£€æŸ¥æ¨¡å‹é…ç½®çš„ id2label å±æ€§\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç°åœ¨æˆ‘ä»¬å¯ä»¥å¾—å‡ºç»“è®ºï¼Œè¯¥æ¨¡å‹é¢„æµ‹äº†ä»¥ä¸‹å‡ ç‚¹ï¼š\n",
    "\n",
    "ç¬¬ä¸€å¥ï¼šå¦å®šï¼š0.0402ï¼Œè‚¯å®šï¼š0.9598\n",
    "\n",
    "ç¬¬äºŒå¥ï¼šå¦å®šï¼š0.9995ï¼Œè‚¯å®šï¼š0.0005\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬å·²ç»æˆåŠŸåœ°å¤åˆ¶äº†ç®¡é“çš„ä¸‰ä¸ªæ­¥éª¤ï¼šä½¿ç”¨æ ‡è®°åŒ–å™¨è¿›è¡Œé¢„å¤„ç†ã€é€šè¿‡æ¨¡å‹ä¼ é€’è¾“å…¥ä»¥åŠåå¤„ç†ï¼ç°åœ¨ï¼Œè®©æˆ‘ä»¬èŠ±ä¸€äº›æ—¶é—´æ·±å…¥äº†è§£è¿™äº›æ­¥éª¤ä¸­çš„æ¯ä¸€æ­¥ã€‚\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ¨¡å‹\n",
    "\n",
    "AutoModel ç±»åŠå…¶æ‰€æœ‰ç›¸å…³é¡¹å®é™…ä¸Šæ˜¯å¯¹åº“ä¸­å„ç§å¯ç”¨æ¨¡å‹çš„ç®€å•åŒ…è£…ã€‚å®ƒæ˜¯ä¸€ä¸ªèªæ˜çš„åŒ…è£…å™¨ï¼Œå› ä¸ºå®ƒå¯ä»¥è‡ªåŠ¨çŒœæµ‹æ£€æŸ¥ç‚¹çš„é€‚å½“æ¨¡å‹ä½“ç³»ç»“æ„ï¼Œç„¶åç”¨è¯¥ä½“ç³»ç»“æ„å®ä¾‹åŒ–æ¨¡å‹ã€‚\n",
    "\n",
    "ä½†æ˜¯ï¼Œå¦‚æœæ‚¨çŸ¥é“è¦ä½¿ç”¨çš„æ¨¡å‹ç±»å‹ï¼Œåˆ™å¯ä»¥ä½¿ç”¨ç›´æ¥å®šä¹‰å…¶ä½“ç³»ç»“æ„çš„ç±»ã€‚è®©æˆ‘ä»¬çœ‹çœ‹è¿™æ˜¯å¦‚ä½•ä¸ `BERT` æ¨¡å‹ä¸€èµ·å·¥ä½œçš„ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/data/study/python/NLP/.env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ä½¿ç”¨ Transformers æ¨¡å‹è¿›è¡Œæ¨ç†\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer æ¨¡å‹åªèƒ½å¤„ç†æ•°å­—â€”â€”æ ‡è®°å™¨(Tokenizer)ç”Ÿæˆçš„æ•°å­—ã€‚æ ‡è®°å™¨(Tokenizer)å¯ä»¥å°†è¾“å…¥è½¬æ¢ä¸ºé€‚å½“çš„æ¡†æ¶å¼ é‡ã€‚\n",
    "\n",
    "æ ‡è®°å™¨(Tokenizer)æ˜¯ NLP ç®¡é“çš„æ ¸å¿ƒç»„ä»¶ä¹‹ä¸€ã€‚å®ƒä»¬æœ‰ä¸€ä¸ªç›®çš„ï¼šå°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥å¤„ç†çš„æ•°æ®ã€‚æ¨¡å‹åªèƒ½å¤„ç†æ•°å­—ï¼Œå› æ­¤æ ‡è®°å™¨(Tokenizer)éœ€è¦å°†æˆ‘ä»¬çš„æ–‡æœ¬è¾“å…¥è½¬æ¢ä¸ºæ•°å­—æ•°æ®ã€‚\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åŠ è½½å’Œä¿å­˜æ ‡è®°å™¨(tokenizer)å°±åƒä½¿ç”¨æ¨¡å‹ä¸€æ ·ç®€å•ã€‚å®é™…ä¸Šï¼Œå®ƒåŸºäºç›¸åŒçš„ä¸¤ç§æ–¹æ³•ï¼š `from_pretrained()` å’Œ `save_pretrained()` ã€‚è¿™äº›æ–¹æ³•å°†åŠ è½½æˆ–ä¿å­˜æ ‡è®°å™¨(tokenizer)ä½¿ç”¨çš„ç®—æ³•ï¼ˆæœ‰ç‚¹åƒå»ºç­‘å­¦(architecture)çš„æ¨¡å‹ï¼‰ä»¥åŠå®ƒçš„è¯æ±‡ï¼ˆæœ‰ç‚¹åƒæƒé‡(weights)æ¨¡å‹ï¼‰ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Using a Transformer network is simple\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ ‡è®°å™¨(Toeknizer)\n",
    "\n",
    "### ç¼–ç \n",
    "\n",
    "å°†æ–‡æœ¬ç¿»è¯‘æˆæ•°å­—è¢«ç§°ä¸ºç¼–ç (encoding). ç¼–ç åˆ†ä¸¤æ­¥å®Œæˆï¼šæ ‡è®°åŒ–ï¼Œç„¶åè½¬æ¢ä¸ºè¾“å…¥ IDã€‚\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### æ ‡è®°åŒ–\n",
    "\n",
    "æ ‡è®°åŒ–è¿‡ç¨‹ç”±æ ‡è®°å™¨(tokenizer)çš„ `tokenize()` æ–¹æ³•å®ç°ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "sequence = \"Using a Transformer network is simple\"\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿™ä¸ªæ ‡è®°å™¨(tokenizer)æ˜¯ä¸€ä¸ªå­è¯æ ‡è®°å™¨(tokenizer)ï¼šå®ƒå¯¹è¯è¿›è¡Œæ‹†åˆ†ï¼Œç›´åˆ°è·å¾—å¯ä»¥ç”¨å…¶è¯æ±‡è¡¨è¡¨ç¤ºçš„æ ‡è®°(token)ã€‚transformer å°±æ˜¯è¿™ç§æƒ…å†µï¼Œå®ƒåˆ†ä¸ºä¸¤ä¸ªæ ‡è®°ï¼štrans å’Œ ##formerã€‚\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ä»è¯ç¬¦(token)åˆ°è¾“å…¥ ID\n",
    "\n",
    "è¾“å…¥ ID çš„è½¬æ¢ç”±æ ‡è®°å™¨(tokenizer)çš„ `convert_tokens_to_ids()` æ–¹æ³•å®ç°ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7993, 170, 13809, 23763, 2443, 1110, 3014]\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(ids)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è§£ç \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a Transformer network is simple\n"
     ]
    }
   ],
   "source": [
    "decoded_string = tokenizer.decode([7993, 170, 13809, 23763, 2443, 1110, 3014])\n",
    "print(decoded_string)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å¤„ç†å¤šä¸ªåºåˆ—\n",
    "\n",
    "### ä»Tokenizerå¾—åˆ°å¼ é‡\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/data/study/python/NLP/.env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input ids: tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
      "          2026,  2878,  2166,  1012]])\n",
      "Logits: tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_ids = torch.tensor([ids])  # å› ä¸ºæ¨¡å‹éœ€è¦ä¼ å…¥ä¸€æ‰¹å¥å­æ‰€å¯¹åº”çš„å¼ é‡ï¼Œæ‰€ä»¥è¿™é‡Œæˆ‘ä»¬ç”¨ä¸€ä¸ªåˆ—è¡¨åŒ…è£¹äº†ä¸€ä¸‹\n",
    "print(\"input ids:\", input_ids)\n",
    "\n",
    "output = model(input_ids)\n",
    "print(\"Logits:\", output.logits)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿™äº›ä¸æ˜¯æ¦‚ç‡ï¼Œè€Œæ˜¯ `logits` ï¼Œå³æ¨¡å‹æœ€åä¸€å±‚è¾“å‡ºçš„åŸå§‹éæ ‡å‡†åŒ–åˆ†æ•°ã€‚è¦è½¬æ¢ä¸ºæ¦‚ç‡ï¼Œå®ƒä»¬éœ€è¦ç»è¿‡ `SoftMax` å±‚ï¼ˆæ‰€æœ‰ ğŸ¤—Transformers æ¨¡å‹è¾“å‡º logitsï¼Œå› ä¸ºç”¨äºè®­ç»ƒçš„æŸè€—å‡½æ•°é€šå¸¸ä¼šå°†æœ€åçš„æ¿€æ´»å‡½æ•°ï¼ˆå¦‚ SoftMaxï¼‰ä¸å®é™…æŸè€—å‡½æ•°ï¼ˆå¦‚äº¤å‰ç†µï¼‰èåˆï¼‰ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0037, 0.9963]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "predictions = torch.nn.functional.softmax(output.logits, dim=-1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœä¼ å…¥å¤šä¸ªå¥å­ï¼Œå¯èƒ½ç±»ä¼¼äºè¿™æ ·ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_ids = [ids, ids]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½†æ˜¯å› ä¸ºå¼ é‡æ˜¯çŸ©å½¢ï¼Œå¦‚æœæˆ‘ä»¬ä¼ å…¥çš„å¥å­é•¿åº¦ä¸ä¸€æ ·ï¼Œéœ€è¦å¡«å……è¾“å…¥åæ‰èƒ½ä¼ å…¥æ¨¡å‹ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200]\n",
    "]  # è¿™ä¸ªåˆ—è¡¨æ˜¯ä¸èƒ½ç›´æ¥è½¬æ¢ä¸ºå¼ é‡çš„ã€‚å› ä¸ºåˆ—è¡¨çš„é•¿åº¦ä¸ä¸€è‡´ï¼Œä¸æ˜¯çŸ©å½¢"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "äº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å¡«å……ä½¿å¼ é‡å…·æœ‰çŸ©å½¢ã€‚Paddingé€šè¿‡åœ¨å€¼è¾ƒå°‘çš„å¥å­ä¸­æ·»åŠ ä¸€ä¸ªåä¸ºPadding tokençš„ç‰¹æ®Šå•è¯æ¥ç¡®ä¿æˆ‘ä»¬æ‰€æœ‰çš„å¥å­é•¿åº¦ç›¸åŒã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 1.5694, -1.3895],\n",
      "        [ 1.3374, -1.2163]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence1_ids = [[200, 200, 200]]\n",
    "sequence2_ids = [[200, 200]]\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "print(model(torch.tensor(sequence1_ids)).logits)\n",
    "print(model(torch.tensor(sequence2_ids)).logits)\n",
    "print(model(torch.tensor(batched_ids)).logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬æ‰¹å¤„ç†é¢„æµ‹ä¸­çš„logitsæœ‰ç‚¹é—®é¢˜ï¼šç¬¬äºŒè¡Œåº”è¯¥ä¸ç¬¬äºŒå¥çš„logitsç›¸åŒï¼Œä½†æˆ‘ä»¬å¾—åˆ°äº†å®Œå…¨ä¸åŒçš„å€¼ï¼\n",
    "\n",
    "è¿™æ˜¯å› ä¸ºTransformeræ¨¡å‹çš„å…³é”®ç‰¹æ€§æ˜¯å…³æ³¨å±‚, å®ƒå…è®¸æ¨¡å‹åœ¨å¤„ç†åºåˆ—æ—¶å…³æ³¨åºåˆ—ä¸­çš„ç‰¹å®šéƒ¨åˆ†ã€‚æˆ‘ä»¬éœ€è¦å‘Šè¯‰è¿™äº›æ³¨æ„å±‚å¿½ç•¥å¡«å……æ ‡è®°ã€‚è¿™æ˜¯é€šè¿‡ä½¿ç”¨ `attention mask` æ¥å®ç°çš„ã€‚\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention mask\n",
    "\n",
    "Attention masksæ˜¯ä¸è¾“å…¥IDå¼ é‡å½¢çŠ¶å®Œå…¨ç›¸åŒçš„å¼ é‡ï¼Œç”¨0å’Œ1å¡«å……ï¼š1sè¡¨ç¤ºåº”æ³¨æ„ç›¸åº”çš„æ ‡è®°ï¼Œ0sè¡¨ç¤ºä¸åº”æ³¨æ„ç›¸åº”çš„æ ‡è®°ï¼ˆå³ï¼Œæ¨¡å‹çš„æ³¨æ„åŠ›å±‚åº”å¿½ç•¥å®ƒä»¬ï¼‰ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5694, -1.3895],\n",
      "        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "attention_mask = [\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 0],\n",
    "]\n",
    "\n",
    "outputs = model(torch.tensor(batched_ids),\n",
    "                attention_mask=torch.tensor(attention_mask))\n",
    "print(outputs.logits)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿™æ ·æˆ‘ä»¬å°±å¾—åˆ°äº†æ­£ç¡®çš„ç»“æœï¼Œä¸ä¸Šé¢ç¬¬äºŒå¥è¯çš„logitsç›¸åŒã€‚\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä½¿ç”¨ Transformers API é«˜çº§å‡½æ•°ç®€åŒ–æ“ä½œ\n",
    "\n",
    "ğŸ¤— Transformers APIå¯ä»¥é€šè¿‡ä¸€ä¸ªé«˜çº§å‡½æ•°ä¸ºæˆ‘ä»¬å¤„ç†æ‰€æœ‰è¿™äº›ï¼Œæˆ‘ä»¬å°†åœ¨è¿™é‡Œæ·±å…¥è®¨è®ºã€‚å½“ä½ ç›´æ¥åœ¨å¥å­ä¸Šè°ƒç”¨æ ‡è®°å™¨æ—¶ï¼Œä½ ä¼šå¾—åˆ°å‡†å¤‡é€šè¿‡æ¨¡å‹ä¼ é€’çš„è¾“å…¥\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "model_inputs = tokenizer(sequence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿™é‡Œï¼Œ `model_inputs` å˜é‡åŒ…å«æ¨¡å‹è‰¯å¥½è¿è¡Œæ‰€éœ€çš„ä¸€åˆ‡ã€‚å¯¹äºDistilBERTï¼Œå®ƒåŒ…æ‹¬è¾“å…¥ IDå’Œæ³¨æ„åŠ›æ©ç ( `attention mask` )ã€‚\n",
    "\n",
    "å®ƒè¿˜ä¸€æ¬¡å¤„ç†å¤šä¸ªåºåˆ—ï¼Œå¹¶ä¸”APIæ²¡æœ‰ä»»ä½•å˜åŒ–ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "model_inputs = tokenizer(sequences)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å®ƒè¿˜ä¸€æ¬¡å®ƒå¯ä»¥æ ¹æ®å‡ ä¸ªç›®æ ‡è¿›è¡Œå¡«å……ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will pad the sequences up to the maximum sequence length\n",
    "model_inputs = tokenizer(sequences, padding=\"longest\")\n",
    "\n",
    "# Will pad the sequences up to the model max length\n",
    "# (512 for BERT or DistilBERT)\n",
    "model_inputs = tokenizer(sequences, padding=\"max_length\")\n",
    "\n",
    "# Will pad the sequences up to the specified max length\n",
    "model_inputs = tokenizer(sequences, padding=\"max_length\", max_length=8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å®ƒè¿˜å¯ä»¥æˆªæ–­åºåˆ—:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "# Will truncate the sequences that are longer than the model max length\n",
    "# (512 for BERT or DistilBERT)\n",
    "model_inputs = tokenizer(sequences, truncation=True)\n",
    "\n",
    "# Will truncate the sequences that are longer than the specified max length\n",
    "model_inputs = tokenizer(sequences, max_length=8, truncation=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ ‡è®°å™¨å¯¹è±¡å¯ä»¥å¤„ç†åˆ°ç‰¹å®šæ¡†æ¶å¼ é‡çš„è½¬æ¢ï¼Œç„¶åå¯ä»¥ç›´æ¥å‘é€åˆ°æ¨¡å‹ã€‚\"pt\"è¿”å› `PyTorch` å¼ é‡ï¼Œ\"tf\"è¿”å› `TensorFlow` å¼ é‡ï¼Œ\"np\"è¿”å› `NumPy` æ•°ç»„ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "# Returns PyTorch tensors\n",
    "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# Returns TensorFlow tensors\n",
    "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"tf\")\n",
    "\n",
    "# Returns NumPy arrays\n",
    "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"np\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### å®Œæ•´å®ä¾‹\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "tokens = tokenizer(sequences, padding=True,\n",
    "                   truncation=True, return_tensors=\"pt\")\n",
    "output = model(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-1.5607,  1.6123],\n",
       "        [-3.6183,  3.9137]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.0195e-02, 9.5981e-01],\n",
      "        [5.3534e-04, 9.9946e-01]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "predictions = torch.nn.functional.softmax(output.logits, dim=-1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
